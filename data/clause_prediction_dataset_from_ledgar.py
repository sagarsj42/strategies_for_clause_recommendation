import sys
import time
import json
import random
from collections import defaultdict

import pandas as pd
from sklearn.model_selection import StratifiedShuffleSplit
from matplotlib import pyplot as plt


def create_contract_label_clause_pairings(ledgar, min_clauses=0, max_clauses=sys.maxsize):
    contr_clauses = dict()
    label_clauses = defaultdict(list)
    label_counts = defaultdict(int)

    for name, group in ledgar.groupby('source'):
        n_clauses = group.shape[0]

        if (n_clauses >= min_clauses) and (n_clauses <= max_clauses):
            lab_clause_list = list()
            
            for labels, clause in zip(group['label'], group['provision']):
                for label in labels:
                    label_clauses[label].append(clause)
                    label_counts[label] += 1
                
                lab_clause_list.append((labels, clause))
            
            contr_clauses[name] = lab_clause_list
            
    return contr_clauses, label_clauses, label_counts


def make_clause_rec_dataset(contr_clauses, top_labels, separate_per_type_occurence=False, 
    pos_only=False, balanced=True):
    
    contracts = list(contr_clauses.keys())
    n_contracts = len(contracts)
    clause_rec = defaultdict(list)
    lab_occurs = False

    for label in list(top_labels):
        print(label, end=' ')
        for contr_name, contract in contr_clauses.items():
            lab_occurs = False
            
            for label_list, clause in contract:
                if label in label_list:
                    lab_occurs = True

            if lab_occurs:
                add_clauses = list()
                rec_clauses = list()
                for label_list, clause in contract:
                    if label not in label_list:
                        add_clauses.append(clause)
                    else:
                        rec_clauses.append(clause)

                if separate_per_type_occurence:
                    for clause in rec_clauses:
                        if pos_only:
                            clause_rec[label].append((add_clauses, clause, contr_name))
                        else:
                            clause_rec[label].append((add_clauses, 1, clause, contr_name))
                else:
                    if pos_only:
                        clause_rec[label].append((add_clauses, rec_clauses, contr_name))
                    else:
                        clause_rec[label].append((add_clauses, 1, rec_clauses, contr_name))

        pos_count = len(clause_rec[label])
        print(pos_count)
        
        if not pos_only:
            neg_count = 0

            for i in random.sample(range(n_contracts), n_contracts):
                contr_name = contracts[i]
                contract = contr_clauses[contr_name]
                lab_occurs = False

                if balanced:
                    if neg_count >= pos_count:
                        break

                for label_list, clause in contract:
                    if label in label_list:
                        lab_occurs = True

                if not lab_occurs:
                    add_clauses = [clause for _, clause in contract]
                    clause_rec[label].append((add_clauses, 0, list(), contr_name))
                    neg_count += 1
                
    return clause_rec


# Needs data generated by make_clause_rec_dataset by having separate_per_type_occurence=False only
# Other arguments can be anything

def convert_clause_rec_to_df(clause_rec, pos_only=True):
    ids = list(range(100000, 999999))
    random.shuffle(ids)
    all_sets = list()
    i = 0

    for label in clause_rec:
        for label_set in clause_rec[label]:
            
            if pos_only:
                clauses, rec_clauses, contr_name = label_set
                num_outputs = len(rec_clauses)
                
                for j, rec_clause in enumerate(rec_clauses):
                    other_ids = [ids[i+k] for k in range(num_outputs) if not i+k == i+j]
                    instance = {
                                'id': ids[i+j], 'contract_clauses': clauses, 
                                'label': label, 'rec_clause': rec_clause, 
                                'other_output_ids': other_ids, 'contract': contr_name
                               }
                    all_sets.append(instance)
                
                i += num_outputs
            else:
                clauses, flag, rec_clauses, contr_name = label_set
                if flag == 1:
                    num_outputs = len(rec_clauses)
                
                    for j, rec_clause in enumerate(rec_clauses):
                        other_ids = [ids[i+k] for k in range(num_outputs) if not i+k == i+j]
                        instance = {
                                    'id': ids[i+j], 'contract_clauses': clauses, 
                                    'label': label, 'rec_clause': rec_clause, 
                                    'other_output_ids': other_ids, 'contract': contr_name
                                   }
                        all_sets.append(instance)

                    i += num_outputs
                    
    clauserec_df = pd.DataFrame.from_records(all_sets, index='id')
                                                        
    return clauserec_df


random.seed(43419)
filename = 'LEDGAR_2016-2019_clean.jsonl'
ledgar = pd.read_json(filename, lines=True)

df_contracts = list(ledgar['source'])
contracts = set()

for contract in df_contracts:
    contracts.add(contract)
    
contracts = list(contracts)
clause_sizes = list()

for name, group in ledgar.groupby('source'):
    no_clauses = group.shape[0]
    clause_sizes.append(no_clauses)

plt.figure(figsize=(16, 9))
plt.hist(clause_sizes, log=True, histtype='bar', facecolor='y')
plt.grid(True, which='both')
plt.xlabel('# clauses in a contract')
plt.ylabel('log[# contracts]')
plt.title('Histogram of clause counts per contract')
plt.show()

rem_counts = defaultdict(int)

for cs in clause_sizes:
    if cs <= 5:
        rem_counts[str(cs)] += 1
        rem_counts['total'] += 1
#     elif cs > 25:
#         rem_counts['>25'] += 1
#         rem_counts['total'] += 1
    else:
        rem_counts['good'] += 1

labels = set()

for name, group in ledgar.groupby('source'):
    n_clauses = group.shape[0]
    
    if n_clauses > 5:
        for label_list in list(group['label']):
            for label in label_list:
                labels.add(label)

labels = list(labels)
        
contr_clauses, label_clauses, label_counts = create_contract_label_clause_pairings(ledgar, min_clauses=6)

with open('contract-clauses.json', 'w') as f:
    json.dump(contr_clauses, f)

with open('label-clauses.json', 'w') as f:
    json.dump(label_clauses, f)

plt.figure(figsize=(16, 9))
plt.hist(label_counts.values(), log=True, color='pink', bins=60)
plt.grid(True, which='both')
plt.xlabel('# clauses for a label')
plt.ylabel('log[# labels]')
plt.title('Histogram of clause counts per label')
plt.show()

top_labels = dict() 

for label, count in label_counts.items():
    if count >= 5000:
        top_labels[label] = count

clause_rec = make_clause_rec_dataset(contr_clauses, top_labels, separate_per_type_occurence=False, 
            pos_only=True, balanced=True)

start = time.time()

with open('clauserec-posonly-unseparated.json', 'w') as f:
    json.dump(clause_rec, f)
    
print('Time to store file:', time.time() - start)

clauses_per_contr = defaultdict(list)

for label, contracts in clause_rec.items():
    for contract in contracts:
        n_clauses = len(contract[1])
        clauses_per_contr[label].append(n_clauses)

print('%2s %30s %25s' % ('Clause type', '# >1 clauses/contract', 'Max # clauses/contract'))
print('-'*70)

total = 0
for label, lst in clauses_per_contr.items():
    count = 0
    max_no = 0
    for nclauses in lst:
        if nclauses > 1:
            count += 1
            total +=1
            if nclauses > max_no:
                max_no = nclauses
    print(f'{label:20s} {count:10d} {max_no:20d}')
print('-'*70)
print('%5s %25d' % ('Total', total))

clauserec_df = convert_clause_rec_to_df(clause_rec, pos_only=True)
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=43419)

for train_index, dev_index in splitter.split(clauserec_df, y=clauserec_df.label):
    train_split = clauserec_df.iloc[train_index]
    dev_split = clauserec_df.iloc[dev_index]
    
print(train_split.label.value_counts(normalize=True), '\n')
print(dev_split.label.value_counts(normalize=True), '\n')
print(train_split.info(), '\n')
print(dev_split.info())

start = time.time()

train_split.to_parquet('clauserec-train.parquet', engine='pyarrow', index=True)
dev_split.to_parquet('clauserec-dev.parquet', engine='pyarrow', index=True)

print('Time to store parquet:', time.time() - start)

start = time.time()

train_split = pd.read_parquet('clauserec-train.parquet', engine='fastparquet')
dev_split = pd.read_parquet('clauserec-dev.parquet', engine='fastparquet')

print('Time to load parquet', time.time() - start)
print(train_split.label.value_counts(normalize=False), '\n')
print(dev_split.label.value_counts(normalize=False))
